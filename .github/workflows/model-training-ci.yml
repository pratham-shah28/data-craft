name: Model Training CI/CD Pipeline

on:
  push:
    paths:
      - 'model-training/**'  # Includes all model-training code, scripts, configs, and data
      - 'model-training/data/user_queries.txt'  # Explicitly trigger on query changes (affects evaluation)
      - '.github/workflows/model-training-ci.yml'
  pull_request:
    paths:
      - 'model-training/**'  # Includes all model-training code, scripts, configs, and data
      - 'model-training/data/user_queries.txt'  # Explicitly trigger on query changes (affects evaluation)
  workflow_dispatch:  # Allows manual triggering

env:
  PYTHON_VERSION: '3.10'
  GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID || 'datacraft-data-pipeline' }}
  GCP_REGION: ${{ secrets.GCP_REGION || 'us-central1' }}

jobs:
  train-and-validate:
    runs-on: ubuntu-latest
    timeout-minutes: 60  # Pipeline should complete within 60 minutes
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install apache-airflow google-cloud-aiplatform pyyaml
      
      - name: Authenticate to GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
      
      - name: Run model training pipeline
        id: training
        run: |
          python model-training/ci-cd/scripts/run_training_pipeline.py --direct
        continue-on-error: true
      
      - name: Validate model metrics
        id: validate
        if: steps.training.outcome == 'success'
        run: |
          python model-training/ci-cd/scripts/validate_model_metrics.py
        continue-on-error: true
      
      - name: Check bias thresholds
        id: bias-check
        if: steps.training.outcome == 'success'
        run: |
          python model-training/ci-cd/scripts/check_bias_thresholds.py
        continue-on-error: true
      
      - name: Compare with previous model
        id: compare
        if: steps.training.outcome == 'success' && steps.validate.outcome == 'success' && steps.bias-check.outcome == 'success'
        run: |
          python model-training/ci-cd/scripts/compare_models.py
        continue-on-error: true
      
      - name: Push to Model Registry
        id: registry
        if: steps.training.outcome == 'success' && steps.validate.outcome == 'success' && steps.bias-check.outcome == 'success' && steps.compare.outcome == 'success'
        run: |
          python model-training/ci-cd/scripts/push_to_registry.py
        continue-on-error: true
      
      - name: Deploy model
        id: deploy
        if: steps.training.outcome == 'success' && steps.validate.outcome == 'success' && steps.bias-check.outcome == 'success' && steps.compare.outcome == 'success' && steps.registry.outcome == 'success'
        run: |
          # Get model resource name from registry output
          # For now, this requires manual intervention or storing in artifact
          echo "Deployment step - requires model resource name from registry step"
          # python model-training/ci-cd/scripts/deploy_model.py <model_resource_name>
        continue-on-error: true
      
      - name: Send success notification
        if: steps.training.outcome == 'success' && steps.validate.outcome == 'success' && steps.bias-check.outcome == 'success' && steps.compare.outcome == 'success'
        run: |
          python model-training/ci-cd/scripts/send_notifications.py success outputs/model-training/pipeline_summary.json
        continue-on-error: true
      
      - name: Send validation failure notification
        if: steps.validate.outcome == 'failure'
        run: |
          python model-training/ci-cd/scripts/send_notifications.py validation_failed
        continue-on-error: true
      
      - name: Send bias failure notification
        if: steps.bias-check.outcome == 'failure'
        run: |
          python model-training/ci-cd/scripts/send_notifications.py bias_failed
        continue-on-error: true
      
      - name: Send comparison failure notification
        if: steps.compare.outcome == 'failure'
        run: |
          python model-training/ci-cd/scripts/send_notifications.py comparison_failed
        continue-on-error: true
      
      - name: Send training failure notification
        if: steps.training.outcome == 'failure'
        run: |
          python model-training/ci-cd/scripts/send_notifications.py failed "Training pipeline execution failed"
        continue-on-error: true
      
      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: model-artifacts
          path: |
            outputs/model-training/**
            outputs/best-model-responses/**
            outputs/evaluation/**
            outputs/bias/**
            outputs/model-selection/**
          retention-days: 30
      
      - name: Summary
        if: always()
        run: |
          echo "## Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Step | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Training | ${{ steps.training.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Validation | ${{ steps.validate.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Bias Check | ${{ steps.bias-check.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Model Comparison | ${{ steps.compare.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Registry Push | ${{ steps.registry.outcome }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Deployment | ${{ steps.deploy.outcome }} |" >> $GITHUB_STEP_SUMMARY

